{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Libraries Installation**"
      ],
      "metadata": {
        "id": "pHbAMv_eCmLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install transformers torch\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "VtU_IK-bBhrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rNuBE9aHH7Bx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e538d5-816c-4d7a-d443-da6ec2ebb32f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Translation**"
      ],
      "metadata": {
        "id": "foDe1Qz4DKsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = \"VietAI/envit5-translation\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Translation Function\n",
        "def translate_text(text, source_lang, target_lang):\n",
        "    prompt = f\"Translate the following text from {source_lang} to {target_lang}: {text}\"\n",
        "\n",
        "    # Tokenize and generate output\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_length=256)\n",
        "\n",
        "    translated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return translated_text\n",
        "\n",
        "# Saving Model\n",
        "path = \"/content/drive/My Drive/Chatbot_Project/Translator_Model\"\n",
        "tokenizer.save_pretrained(path)\n",
        "model.save_pretrained(path)\n",
        "print(f\"Model Saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nK516nF_eOYW",
        "outputId": "1a1a1376-d36c-40e8-f1f2-e64ff7e2007b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question-Answering**"
      ],
      "metadata": {
        "id": "_Xn0X8TjDP_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "import gradio as gr\n",
        "\n",
        "# Load the model and tokenizer (Public & Free)\n",
        "model_name = \"atharvamundada99/bert-large-question-answering-finetuned-legal\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "# Question Answering Function\n",
        "def answer_question(question, context):\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    answer_start = torch.argmax(outputs.start_logits)\n",
        "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
        "\n",
        "    return answer if answer.strip() else \"Sorry, I couldn't find a relevant answer.\"\n",
        "\n",
        "#Saving Model\n",
        "path=\"/content/drive/My Drive/Chatbot_Project/QA_Model\"\n",
        "tokenizer.save_pretrained(path)\n",
        "model.save_pretrained(path)\n",
        "\n",
        "print(f\"Model Saved\")"
      ],
      "metadata": {
        "id": "HCatafYEDUR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50430c59-7381-411d-b009-215b7838eb6c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Summarization**"
      ],
      "metadata": {
        "id": "qj_UIokkDdMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
        "# Load the model and tokenizer (Public & Free)\n",
        "model_name = \"Falconsai/medical_summarization\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "# Summarization Function\n",
        "def summarize_text(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        summary_ids = model.generate(**inputs, max_length=150, min_length=50, length_penalty=2.0, num_beams=4)\n",
        "\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Saving Model\n",
        "path=\"/content/drive/My Drive/Chatbot_Project/Summarization_Model\"\n",
        "tokenizer.save_pretrained(path)\n",
        "model.save_pretrained(path)\n",
        "\n",
        "print(f\"Model Saved\")"
      ],
      "metadata": {
        "id": "rtZs6NatDkwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d9dda23-f18a-492e-9a02-3000e0db4dc0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GUI**"
      ],
      "metadata": {
        "id": "-kj9RO6Sguhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Function to toggle UI visibility based on selected task\n",
        "def select_task(task):\n",
        "    return (\n",
        "        gr.update(visible=(task == \"Translation\")),\n",
        "        gr.update(visible=(task == \"Question Answering\")),\n",
        "        gr.update(visible=(task == \"Summarization\")),\n",
        "    )\n",
        "\n",
        "# Function to clear inputs and outputs\n",
        "def clear_fields():\n",
        "    return \"\", \"\", \"\", \"\"\n",
        "\n",
        "def clear_fields_summary():\n",
        "    return \"\"\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Select Task\")\n",
        "\n",
        "    task_buttons = gr.Radio([\"Translation\", \"Question Answering\", \"Summarization\"], label=\"Choose a task\")\n",
        "\n",
        "    with gr.Group(visible=False) as translation_ui:\n",
        "        source_lang = gr.Textbox(label=\"Source Language\")\n",
        "        target_lang = gr.Textbox(label=\"Target Language\")\n",
        "        text_input = gr.Textbox(label=\"Enter Text\")\n",
        "        translate_button = gr.Button(\"Translate\")\n",
        "        translation_output = gr.Textbox(label=\"Translated Text\")\n",
        "\n",
        "        # Clear button for Translation\n",
        "        clear_button_t = gr.Button(\"Clear\")\n",
        "        clear_button_t.click(clear_fields, inputs=[], outputs=[source_lang, target_lang, text_input, translation_output])\n",
        "\n",
        "        translate_button.click(translate_text, inputs=[text_input, source_lang, target_lang], outputs=translation_output)\n",
        "\n",
        "    with gr.Group(visible=False) as qa_ui:\n",
        "        question_input = gr.Textbox(label=\"Enter Question\")\n",
        "        context_input = gr.Textbox(label=\"Enter Context\")\n",
        "        answer_button = gr.Button(\"Get Answer\")\n",
        "        qa_output = gr.Textbox(label=\"Answer\")\n",
        "\n",
        "        # Clear button for Question Answering\n",
        "        clear_button_qa = gr.Button(\"Clear\")\n",
        "        clear_button_qa.click(clear_fields, inputs=[], outputs=[question_input, context_input, qa_output])\n",
        "\n",
        "        answer_button.click(answer_question, inputs=[question_input, context_input], outputs=qa_output)\n",
        "\n",
        "    with gr.Group(visible=False) as summarization_ui:\n",
        "        text_input_summary = gr.Textbox(label=\"Enter Text\")\n",
        "        summarize_button = gr.Button(\"Summarize\")\n",
        "        summary_output = gr.Textbox(label=\"Summary\")\n",
        "\n",
        "        # Clear button for Summarization\n",
        "        clear_button_s = gr.Button(\"Clear\")\n",
        "        clear_button_s.click(clear_fields_summary, inputs=[], outputs=[text_input_summary, summary_output])\n",
        "\n",
        "        summarize_button.click(summarize_text, inputs=[text_input_summary], outputs=summary_output)\n",
        "\n",
        "    # Link the task selection with visibility updates\n",
        "    task_buttons.change(select_task, inputs=[task_buttons], outputs=[translation_ui, qa_ui, summarization_ui])"
      ],
      "metadata": {
        "id": "0jVVSKcXgjOP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch()"
      ],
      "metadata": {
        "id": "15cvXnlXirKe",
        "outputId": "efc1175f-9e04-4680-9ebe-29e9373123ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://196cbe66c63c33d535.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://196cbe66c63c33d535.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}